# Simplified ML Pipeline Architecture (Cloud Run + Vertex AI)

## Overview

Split workload between **Cloud Run (orchestrator)** and **Vertex AI Custom Training (heavy ML)**

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cloud Run (Lightweight Orchestrator)                       â”‚
â”‚                                                             â”‚
â”‚ Container: orchestrator/                                   â”‚
â”‚ â”œâ”€â”€ main.py              Flask app (HTTP endpoint)         â”‚
â”‚ â”œâ”€â”€ orchestrator.py      Orchestration logic               â”‚
â”‚ â”œâ”€â”€ config.yaml          Configuration                     â”‚
â”‚ â””â”€â”€ Dockerfile           Lightweight image                 â”‚
â”‚                                                             â”‚
â”‚ What it does:                                              â”‚
â”‚ 1. Authenticate to Earth Engine + GCS                      â”‚
â”‚ 2. Trigger Earth Engine export â†’ GCS                       â”‚
â”‚ 3. Call Vertex AI Training Job API                         â”‚
â”‚ 4. Monitor training completion                             â”‚
â”‚ 5. Evaluate results & deploy if gates pass                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“ triggers
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vertex AI Custom Training (Heavy ML Workload)              â”‚
â”‚                                                             â”‚
â”‚ Container: trainer/                                        â”‚
â”‚ â”œâ”€â”€ train.py             Training script                   â”‚
â”‚ â”œâ”€â”€ Dockerfile           ML-optimized image                â”‚
â”‚ â””â”€â”€ requirements.txt     ML libraries (sklearn, pandas)    â”‚
â”‚                                                             â”‚
â”‚ What it does:                                              â”‚
â”‚ 1. Load training data from GCS                             â”‚
â”‚ 2. Train RandomForest model                                â”‚
â”‚ 3. Save model artifact to GCS                              â”‚
â”‚ 4. Return metrics to Vertex AI                             â”‚
â”‚                                                             â”‚
â”‚ Runs on: Managed infrastructure (can use GPUs/TPUs!)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Folder Structure

```
ml_pipeline/
â”œâ”€â”€ orchestrator/           # Cloud Run orchestrator
â”‚   â”œâ”€â”€ main.py            # Flask app + HTTP endpoint
â”‚   â”œâ”€â”€ orchestrator.py    # Orchestration logic
â”‚   â”œâ”€â”€ config.yaml        # Configuration
â”‚   â”œâ”€â”€ Dockerfile         # Lightweight image
â”‚   â”œâ”€â”€ requirements.txt   # Minimal deps (no ML libs)
â”‚   â””â”€â”€ deploy.sh          # Deploy orchestrator
â”‚
â”œâ”€â”€ trainer/               # Vertex AI training container
â”‚   â”œâ”€â”€ train.py          # Training script
â”‚   â”œâ”€â”€ Dockerfile        # ML-optimized image
â”‚   â”œâ”€â”€ requirements.txt  # ML libraries
â”‚   â””â”€â”€ build.sh          # Build & push to Artifact Registry
â”‚
â””â”€â”€ config.yaml           # Shared configuration
```

## How It Works

### Step 1: Cloud Run Orchestrator Starts

```bash
curl -X POST https://ml-pipeline-xxxxx.run.app
```

**Orchestrator does:**
1. Load `config.yaml` from Cloud Storage
2. Authenticate to Earth Engine
3. Export training data from Earth Engine â†’ Cloud Storage
4. Call Vertex AI Training Job API

### Step 2: Vertex AI Runs Training

**Vertex AI does:**
1. Spins up training container on managed infrastructure
2. Runs `train.py` script
3. Script loads data from GCS
4. Trains model
5. Saves model to GCS
6. Returns metrics

### Step 3: Orchestrator Completes

**Orchestrator does:**
1. Waits for training completion
2. Loads model metrics
3. Evaluates against quality gates
4. Deploys to Vertex AI endpoint if gates pass
5. Returns final status

## Benefits

### âœ… Separation of Concerns
- Cloud Run: Orchestration only (lightweight, cheap)
- Vertex AI: Heavy ML training (optimized infrastructure)

### âœ… Cost Optimization
- Cloud Run: Pay only for orchestration time (~1-2 minutes)
- Vertex AI: Pay only for training time (~5-10 minutes)
- No idle time costs

### âœ… Scalability
- Can use GPUs for training if needed
- Vertex AI handles resource management
- Training container can scale independently

### âœ… Simplicity
- One API call triggers everything
- Clear separation between orchestration and training
- Easy to debug (separate logs for each component)

## Deployment

### 1. Build & Push Training Container

```bash
cd trainer
./build.sh  # Builds and pushes to Artifact Registry
```

### 2. Deploy Orchestrator to Cloud Run

```bash
cd orchestrator
./deploy.sh  # Deploys orchestrator
```

### 3. Run Pipeline

```bash
curl -X POST https://ml-pipeline-xxxxx.run.app
```

**That's it!** One HTTP call triggers the entire pipeline.

## Configuration

**Single `config.yaml` for both containers:**

```yaml
project:
  id: ml-pipeline-477612
  region: us-central1

training:
  machine_type: n1-standard-4  # Or n1-highmem-8, n1-standard-16, etc.
  accelerator_type: null       # Or "NVIDIA_TESLA_T4" for GPU
  accelerator_count: 0

quality_gates:
  absolute_min_accuracy: 0.75
  min_per_crop_f1: 0.70
```

## Cost Estimate

- **Cloud Run orchestrator:** ~$0.10/run (1-2 minutes)
- **Vertex AI training:** ~$0.50-2.00/run (5-10 minutes on n1-standard-4)
- **Total per month:** ~$1-3/month (runs once monthly)

**vs Old Architecture:**
- Cloud Functions: ~$5-7/month
- **Savings: 60-70%!**

## Next Steps

1. Create `orchestrator/` folder with lightweight orchestration code
2. Create `trainer/` folder with ML training code  
3. Build and deploy both containers
4. Test end-to-end pipeline

**Much cleaner and more scalable!** ğŸš€

